{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different NLP frameworks that you're likely to encounter. The most popular and widely-used of these are:\n",
    "\n",
    "- ```NLTK``` (Natural Language Toolkit, old-school)\n",
    "- ```UDPipe``` (Neural network based, fast and light, but not super accurate)\n",
    "- ```CoreNLP``` and ```stanza``` (Created by the team at Stanford; academically robust)\n",
    "- ```spaCy``` production-ready, well-documented, state-of-the-art\n",
    "\n",
    "We'll be working with ```spaCy``` in this module, primarily because it's easy and intuitive, and also scales well.\n",
    "\n",
    "First thing we need to do is install ```spaCy``` and the language model that we want to use.\n",
    "\n",
    "From the command line, you should first make sure to run the setup script to install requirements:\n",
    "\n",
    "```shell \n",
    "bash setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ```spaCy```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is import ```spaCy``` __and__ the language model that we want to use.\n",
    "\n",
    "Note that, if you want to use different langauges you want to use different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spacy NLP class\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model now loaded, we can begin to do some very simple NLP tasks.\n",
    "\n",
    "Here, we create a spaCy object and assign it to the variable ```nlp```. This is the NLP pipeline that will do all our heavy lifting, using the trained model we've specified.\n",
    "\n",
    "Below, you can see what the pipeline does with a bit of sample text. Passing text to the nlp object gives us access to a bunch of properties, including tokens (words), parts of speech, named entities, and so on. Here's we two of them, tokens and entities. These objects, in turn, have certain methods attached to them. A full outline of available methods can be found in the spaCy docs.\n",
    "\n",
    "In this case, for all token objects, let's return the token itself (```token.text```); its part-of-speech tag (```token.pos_```); and the grammatical dependency relations between the tokens (```token.dep_```).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single sentence example\n",
    "doc = nlp(\"I like to eat pizza and pasta. So does Tony and Mark who works at Google.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tokenize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "like\n",
      "to\n",
      "eat\n",
      "pizza\n",
      "and\n",
      "pasta\n",
      ".\n",
      "So\n",
      "does\n",
      "Tony\n",
      "and\n",
      "Mark\n",
      "who\n",
      "works\n",
      "at\n",
      "Google\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# tokenizing text\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Trying some more attributes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "like VERB ROOT\n",
      "to PART aux\n",
      "eat VERB xcomp\n",
      "pizza NOUN dobj\n",
      "and CCONJ cc\n",
      "pasta NOUN conj\n",
      ". PUNCT punct\n",
      "So ADV advmod\n",
      "does VERB ROOT\n",
      "Tony PROPN nsubj\n",
      "and CCONJ cc\n",
      "Mark PROPN conj\n",
      "who PRON nsubj\n",
      "works VERB relcl\n",
      "at ADP prep\n",
      "Google PROPN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# find parts-of-speech and grammatical relations\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NER__\n",
    "\n",
    "Extracting named entities from a ```spaCy``` doc requires an extra step, but nothing too challenging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony PERSON\n",
      "Mark PERSON\n",
      "Google ORG\n"
     ]
    }
   ],
   "source": [
    "# extracting NER in doc\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "\n",
    "1. What range of linguistic features is available beyond what we're looking at here? \n",
    "2. Are the same range of features available for all languages? Compare e.g. English and Danish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count distribution of linguistic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create doc object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file\n",
    "import os\n",
    "path = os.path.join(\"..\", \"data\", \"Dickens_Expectations_1861.txt\")\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc object\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# count number of adjectives\u001b[39;00m\n\u001b[1;32m      2\u001b[0m adj_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc:\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mpos_ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mADJ\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m         adj_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# count number of adjectives\n",
    "adj_count = 0\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        adj_count += 1\n",
    "\n",
    "print(\"Number of adjectives:\", adj_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relative frequency__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative frequency per 10,000 words: 456.9674647022713\n"
     ]
    }
   ],
   "source": [
    "# find the relative frequency per 10,000 words\n",
    "adj_freq = adj_count / len(doc) * 10000\n",
    "print(\"Relative frequency per 10,000 words:\", adj_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating neater outputs using ```pandas```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, all of our output from ```spaCy``` is in the form of lists. If we want to save these, it probably makes sense to have them saved in a more transferable format, such as CSV files or JSONs.\n",
    "\n",
    "One very easy way to do this with Python is by using the dataframe library ```pandas```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spaCy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy doc to pandas dataframe\n",
    "df = pd.DataFrame([[token.text, token.pos_, token.dep_, token.head.text, token.head.pos_, [child for child in token.children]] for token in doc], columns=[\"text\", \"pos\", \"dep\", \"head_text\", \"head_pos\", \"children\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_csv(\"Dickens_Expectations_1861.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "      <th>head_text</th>\n",
       "      <th>head_pos</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿REAT</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>EXPECTATIONS</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXPECTATIONS</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>EXPECTATIONS</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[﻿REAT, \\n , Edition, I, name, ,, and, make, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>dep</td>\n",
       "      <td>EXPECTATIONS</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1867</td>\n",
       "      <td>NUM</td>\n",
       "      <td>compound</td>\n",
       "      <td>Edition</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edition</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>appos</td>\n",
       "      <td>EXPECTATIONS</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>[1867, \\n, by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244345</th>\n",
       "      <td>parting</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>[another]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244346</th>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>saw</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[her]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244347</th>\n",
       "      <td>her</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pobj</td>\n",
       "      <td>from</td>\n",
       "      <td>ADP</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244348</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "      <td>saw</td>\n",
       "      <td>VERB</td>\n",
       "      <td>[\\n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244349</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>dep</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244350 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                text    pos       dep     head_text head_pos  \\\n",
       "0              ﻿REAT  PROPN  compound  EXPECTATIONS    PROPN   \n",
       "1       EXPECTATIONS  PROPN      ROOT  EXPECTATIONS    PROPN   \n",
       "2                \\n   SPACE       dep  EXPECTATIONS    PROPN   \n",
       "3               1867    NUM  compound       Edition    PROPN   \n",
       "4            Edition  PROPN     appos  EXPECTATIONS    PROPN   \n",
       "...              ...    ...       ...           ...      ...   \n",
       "244345       parting   NOUN      pobj            of      ADP   \n",
       "244346          from    ADP      prep           saw     VERB   \n",
       "244347           her   PRON      pobj          from      ADP   \n",
       "244348             .  PUNCT     punct           saw     VERB   \n",
       "244349            \\n  SPACE       dep             .    PUNCT   \n",
       "\n",
       "                                               children  \n",
       "0                                                    []  \n",
       "1       [﻿REAT, \\n , Edition, I, name, ,, and, make, .]  \n",
       "2                                                    []  \n",
       "3                                                    []  \n",
       "4                                        [1867, \\n, by]  \n",
       "...                                                 ...  \n",
       "244345                                        [another]  \n",
       "244346                                            [her]  \n",
       "244347                                               []  \n",
       "244348                                             [\\n]  \n",
       "244349                                               []  \n",
       "\n",
       "[244350 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "Spend some time exploring and familiarizing yourself with ```spaCy``` and ```pandas```. We'll come back to them quite a lot through this semester, so it will help to have a solid handle on how they function.\n",
    "\n",
    "When you are ready, head over to [Assignment 1](https://classroom.github.com/a/PdNi7nPv) which takes some of the skills you've learned last week and today. The task will be to count how many times certain linguistic features occur accross different documents, and to save those results in a clear and easy-to-understand way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data type of variable in df\n",
    "df.dtypes\n",
    "\n",
    "# converting variable to string\n",
    "df[\"text\"] = df[\"text\"].astype(str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
